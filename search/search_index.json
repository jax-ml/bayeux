{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Bayeux","text":"<p>Stitching together models and samplers</p> <p> </p> <p><code>bayeux</code> lets you write a probabilistic model in JAX and immediately have access to state-of-the-art inference methods. The API aims to be simple, self descriptive, and helpful. Simply provide a log density function (which doesn't even have to be normalized), along with a single point (specified as a pytree) where that log density is finite. Then let <code>bayeux</code> do the rest!</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install bayeux-ml\n</code></pre>"},{"location":"#quickstart","title":"Quickstart","text":"<p>We define a model by providing a log density in JAX. This could be defined using a probabilistic programming language (PPL) like numpyro, PyMC, TFP, distrax, oryx, coix, or directly in JAX.</p> <pre><code>import bayeux as bx\nimport jax\n\nnormal_density = bx.Model(\n  log_density=lambda x: -x*x,\n  test_point=1.)\n\nseed = jax.random.key(0)\n\nopt_results = normal_density.optimize.optax_adam(seed=seed)\n# OR!\nidata = normal_density.mcmc.numpyro_nuts(seed=seed)\n# OR!\nsurrogate_posterior, loss = normal_density.vi.tfp_factored_surrogate_posterior(seed=seed)\n</code></pre>"},{"location":"#read-more","title":"Read more","text":"<ul> <li>Defining models</li> <li>Inspecting models</li> <li>Testing and debugging</li> <li>Also see <code>bayeux</code> integration with numpyro, PyMC, and TFP!</li> </ul> <p>This is not an officially supported Google product.</p>"},{"location":"debug_mode/","title":"Debug Mode","text":"<p>Algorithms come with a built-in <code>debug</code> mode that attempts to fail quickly and in a manner that might help debug problems quickly. The signature for <code>debug</code> accepts <code>verbosity</code> and <code>catch_exceptions</code> arguments, as well as a <code>kwargs</code> dictionary that the user plans to pass to the algorithm itself.</p>"},{"location":"debug_mode/#default-behavior","title":"Default behavior","text":"<p>By default, debug mode will print a little description of what is happening, and whether the test passed. This can also be useful when unit testing your models, since the return value is whether all the tests passed!</p> <pre><code>import bayeux as bx\nimport jax\nimport jax.numpy as jnp\n\nnormal_density = bx.Model(\n  log_density=lambda x: -x*x,\n  test_point=1.)\n\nseed = jax.random.key(0)\n\nnormal_density.mcmc.numpyro_nuts.debug(seed=seed)\n\nChecking test_point shape \u2713\nComputing test point log density \u2713\nLoading keyword arguments... \u2713\nChecking it is possible to compute an initial state \u2713\nChecking initial state is has no NaN \u2713\nComputing initial state log density \u2713\nTransforming model to R^n \u2713\nComputing transformed state log density shape \u2713\nComparing transformed log density to untransformed \u2713\nComputing gradients of transformed log density \u2713\nTrue\n</code></pre>"},{"location":"debug_mode/#do-not-catch-exceptions","title":"Do not catch exceptions","text":"<p>Often our models are bad because they don't even run. Debug mode aggresively catches exceptions, but you can disable that to make sure it is possible to use the model.</p> <p>See if you can spot what is wrong with this model:</p> <pre><code>bad_model = bx.Model(\n    log_density=lambda x: jnp.sqrt(x['mean']),\n    test_point=-1.)\n\nbad_model.mcmc.numpyro_nuts.debug(seed=seed, catch_exceptions=False)\n\nChecking test_point shape \u2713\nComputing test point log density \u00d7\n      ...\n      1 bad_model = bx.Model(\n----&gt; 2     log_density=lambda x: jnp.sqrt(x['mean']),\n      3     test_point=-1.)\n\nTypeError: 'float' object is not subscriptable\n</code></pre>"},{"location":"debug_mode/#changing-verbosity","title":"Changing verbosity","text":"<p>Debug mode also accepts a <code>verbosity</code> argument. The default is 2. We have a new subtly poorly specified <code>bad_model</code> with no outputs:</p> <pre><code>bad_model = bx.Model(\n    log_density=jnp.sqrt,\n    test_point=-1.)\n\nbad_model.mcmc.blackjax_nuts.debug(seed=seed, verbosity=0, kwargs={\"num_chains\": 17})\n\nFalse\n</code></pre> <p>With <code>verbosity=1</code> there is a minimal output:</p> <pre><code>bad_model.mcmc.blackjax_nuts.debug(seed=seed, verbosity=0, kwargs={\"num_chains\": 17})\n\n\u2713 \u00d7 \u2713 \u2713 \u2713 \u00d7 \u2713 \u2713 \u00d7\nFalse\n</code></pre> <p>With higher verbosity, we can see the actual outputs and perhaps diagnose the problem after seeing that the log density of the initial point is <code>nan</code>. We should have passed in a <code>transform=jnp.exp</code> or similar!:</p> <pre><code>bad_model.mcmc.blackjax_nuts.debug(seed=seed, verbosity=3, kwargs={\"num_chains\": 17})\n\nChecking test_point shape \u2713\nTest point has shape\n()\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nComputing test point log density \u00d7\nTest point has log density\nArray(nan, dtype=float32, weak_type=True)\n\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\n\nLoading keyword arguments... \u2713\nKeyword arguments are\n{&lt;function window_adaptation at 0x14bd62b90&gt;: {'algorithm': &lt;class 'blackjax.mcmc.nuts.nuts'&gt;,\n                                               'initial_step_size': 1.0,\n                                               'is_mass_matrix_diagonal': True,\n                                               'logdensity_fn': &lt;function constrain.&lt;locals&gt;.wrap_log_density.&lt;locals&gt;.wrapped at 0x15fb97880&gt;,\n                                               'progress_bar': False,\n                                               'target_acceptance_rate': 0.8},\n 'adapt.run': {'num_steps': 500},\n 'extra_parameters': {'chain_method': 'vectorized',\n                      'num_adapt_draws': 500,\n                      'num_chains': 17,\n                      'num_draws': 500,\n                      'return_pytree': False},\n &lt;class 'blackjax.mcmc.nuts.nuts'&gt;: {'divergence_threshold': 1000,\n                                     'integrator': &lt;function generate_euclidean_integrator.&lt;locals&gt;.euclidean_integrator at 0x14bad0e50&gt;,\n                                     'logdensity_fn': &lt;function constrain.&lt;locals&gt;.wrap_log_density.&lt;locals&gt;.wrapped at 0x15fb97880&gt;,\n                                     'max_num_doublings': 10,\n                                     'step_size': 0.5}}\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nChecking it is possible to compute an initial state \u2713\nInitial state has shape\n(17,)\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nChecking initial state is has no NaN \u2713\nNo nans detected!\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nComputing initial state log density \u00d7\nInitial state has log density\nArray([1.2212421 ,        nan,        nan, 1.4113309 ,        nan,\n              nan,        nan,        nan,        nan,        nan,\n       0.5912253 ,        nan,        nan,        nan, 0.65457666,\n              nan,        nan], dtype=float32)\n\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\n\nTransforming model to R^n \u2713\nTransformed state has shape\n(17,)\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nComputing transformed state log density shape \u2713\nTransformed state log density has shape\n(17,)\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nComputing gradients of transformed log density \u00d7\nThe gradient contains NaNs! Initial gradients has shape\n(17,)\n\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\u00d7\n\nFalse\n</code></pre> <p>Even bigger numbers will give even more details.</p>"},{"location":"debug_mode/#fun-mode","title":"Fun mode","text":"<p>I mean, you're reading about debugging statistical models.</p> <pre><code>bx.debug.FunMode.engaged = True\n\nbad_model.mcmc.blackjax_nuts.debug(seed=seed, verbosity=1, kwargs={\"num_chains\": 17})\n\n\ud83c\udf08 \ud83d\udc4e \ud83d\udcaa \ud83d\ude4c \ud83d\ude80 \ud83d\udc80 \ud83c\udf08 \u2713 \u274c\nFalse\n</code></pre>"},{"location":"inference/","title":"Building models","text":"<p>The two main contracts <code>bayeux</code> has are that 1. You can specify a model using a log density, a test point, and a transformation (the transformation defaults to an identity, but that is rarely what you want) 2. Every inference algorithm in <code>bayeux</code> will (try to) run with just a seed as an argument.</p>"},{"location":"inference/#specifying-a-model","title":"Specifying a model","text":"<p>In case you have a scalar model, there is no need to normalize the density.</p> <pre><code>import bayeux as bx\nimport jax\nimport numpy as np\n\nnormal_density = bx.Model(\n  log_density=lambda x: -x*x,\n  test_point=1.)\n</code></pre> <p>Suppose we have a bunch of observations of a normal distribution, and we want to infer the mean and scale. Maybe we write this down by hand, putting a prior of N(0, 10) on the mean and half normal with scale 10 on the scale:</p> <pre><code>points = 3 * np.random.randn(100) - 10\n\ndef log_density(pt):\n    log_prior = -(pt['loc'] ** 2 + pt['scale']**2) / 200.\n    log_likelihood = jnp.sum(jst.norm.logpdf(points, loc=pt['loc'], scale=pt['scale']))\n    return log_prior + log_likelihood\n</code></pre> <p>We additionally need to restrict the scale to be positive. A softplus is useful for this:</p> <pre><code>def transform_fn(pt):\n  return {'loc': pt['loc'], 'scale': jax.nn.softplus(pt['scale'])}\n</code></pre> <p>The oryx library is used to automatically compute the inverse and Jacobian determinants for changes of variables, but the user can supply these if known.</p> <p>Then we can get the model: <pre><code>model = bx.Model(\n    log_density=log_density,\n    test_point={'loc': 0., 'scale': 1.},\n    transform_fn=transform_fn)\n\nopt = model.optimize.optax_adam(seed=seed, num_iters=10000)\nopt.params\n\n{'loc': Array([-9.428163, -9.428162, -9.428163, -9.428162, -9.428165, -9.428163,\n        -9.428163, -9.428164], dtype=float32),\n 'scale': Array([2.9746027, 2.9746041, 2.9746022, 2.9746022, 2.9745977, 2.9746022,\n        2.9746027, 2.9746022], dtype=float32)}\n</code></pre></p> <p>By default, we ran 8 particles for optimization, which is helpful to see that all of them found approximately the same maximum likelihood estimate.</p>"},{"location":"inspecting/","title":"Inspecting models","text":""},{"location":"inspecting/#seeing-keyword-arguments","title":"Seeing keyword arguments","text":"<p>Since <code>bayeux</code> is built on top of other fantastic libraries, it tries not to get in the way of them. Each algorithm has a <code>.get_kwargs()</code> method that tells you how it will be called, and what functions are being called:</p> <pre><code>normal_density.optimize.jaxopt_bfgs.get_kwargs()\n\n{jaxopt._src.bfgs.BFGS: {'value_and_grad': False,\n  'has_aux': False,\n  'maxiter': 500,\n  'tol': 0.001,\n  'stepsize': 0.0,\n  'linesearch': 'zoom',\n  'linesearch_init': 'increase',\n  'condition': None,\n  'maxls': 30,\n  'decrease_factor': None,\n  'increase_factor': 1.5,\n  'max_stepsize': 1.0,\n  'min_stepsize': 1e-06,\n  'implicit_diff': True,\n  'implicit_diff_solve': None,\n  'jit': True,\n  'unroll': 'auto',\n  'verbose': False},\n 'extra_parameters': {'chain_method': 'vectorized',\n  'num_particles': 8,\n  'num_iters': 1000,\n  'apply_transform': True}}\n</code></pre> <p>If you pass an argument into <code>.get_kwargs()</code>, this will also tell you what will be passed on to the actual algorithms.</p> <pre><code>normal_density.mcmc.blackjax_nuts.get_kwargs(\n    num_chains=5,\n    target_acceptance_rate=0.99)\n\n{&lt;blackjax.adaptation.window_adaptation.window_adaptation: {'is_mass_matrix_diagonal': True,\n  'initial_step_size': 1.0,\n  'target_acceptance_rate': 0.99,\n  'progress_bar': False,\n  'algorithm': blackjax.mcmc.nuts.nuts},\n blackjax.mcmc.nuts.nuts: {'max_num_doublings': 10,\n  'divergence_threshold': 1000,\n  'integrator': blackjax.mcmc.integrators.velocity_verlet,\n  'step_size': 0.01},\n 'extra_parameters': {'chain_method': 'vectorized',\n  'num_chains': 5,\n  'num_draws': 500,\n  'num_adapt_draws': 500,\n  'return_pytree': False}}\n</code></pre>"},{"location":"inspecting/#available-algorithms","title":"Available algorithms","text":"<p>Algorithms are sometimes dynamically determined at runtime, based on the libraries that are installed (\"pay for what you need\"). A model can give programmatic access to the available algorithms via <code>methods</code>:</p> <pre><code>normal_model.methods\n\n{'mcmc': ['tfp_hmc',\n  'tfp_nuts',\n  'tfp_snaper_hmc',\n  'blackjax_hmc',\n  'blackjax_chees_hmc',\n  'blackjax_meads_hmc',\n  'blackjax_nuts',\n  'blackjax_hmc_pathfinder',\n  'blackjax_nuts_pathfinder',\n  'flowmc_rqspline_hmc',\n  'flowmc_rqspline_mala',\n  'flowmc_realnvp_hmc',\n  'flowmc_realnvp_mala',\n  'numpyro_hmc',\n  'numpyro_nuts'],\n 'optimize': ['jaxopt_bfgs',\n  'jaxopt_gradient_descent',\n  'jaxopt_lbfgs',\n  'jaxopt_nonlinear_cg',\n  'optimistix_bfgs',\n  'optimistix_chord',\n  'optimistix_dogleg',\n  'optimistix_gauss_newton',\n  'optimistix_indirect_levenberg_marquardt',\n  'optimistix_levenberg_marquardt',\n  'optimistix_nelder_mead',\n  'optimistix_newton',\n  'optimistix_nonlinear_cg',\n  'optax_adabelief',\n  'optax_adafactor',\n  'optax_adagrad',\n  'optax_adam',\n  'optax_adamw',\n  'optax_adamax',\n  'optax_amsgrad',\n  'optax_fromage',\n  'optax_lamb',\n  'optax_lion',\n  'optax_noisy_sgd',\n  'optax_novograd',\n  'optax_radam',\n  'optax_rmsprop',\n  'optax_sgd',\n  'optax_sm3',\n  'optax_yogi'],\n 'vi': ['tfp_factored_surrogate_posterior']}\n</code></pre> <p>The string representation of a model will tell you what methods are available.</p> <pre><code>print(normal_density)\n\nmcmc\n    .tfp_hmc\n    .tfp_nuts\n    .tfp_snaper_hmc\n    .blackjax_hmc\n    .blackjax_chees_hmc\n    .blackjax_meads_hmc\n    .blackjax_nuts\n    .blackjax_hmc_pathfinder\n    .blackjax_nuts_pathfinder\n    .flowmc_rqspline_hmc\n    .flowmc_rqspline_mala\n    .flowmc_realnvp_hmc\n    .flowmc_realnvp_mala\n    .numpyro_hmc\n    .numpyro_nuts\noptimize\n    .jaxopt_bfgs\n    .jaxopt_gradient_descent\n    .jaxopt_lbfgs\n    .jaxopt_nonlinear_cg\n    .optimistix_bfgs\n    .optimistix_chord\n    .optimistix_dogleg\n    .optimistix_gauss_newton\n    .optimistix_indirect_levenberg_marquardt\n    .optimistix_levenberg_marquardt\n    .optimistix_nelder_mead\n    .optimistix_newton\n    .optimistix_nonlinear_cg\n    .optax_adabelief\n    .optax_adafactor\n    .optax_adagrad\n    .optax_adam\n    .optax_adamw\n    .optax_adamax\n    .optax_amsgrad\n    .optax_fromage\n    .optax_lamb\n    .optax_lion\n    .optax_noisy_sgd\n    .optax_novograd\n    .optax_radam\n    .optax_rmsprop\n    .optax_sgd\n    .optax_sm3\n    .optax_yogi\nvi\n    .tfp_factored_surrogate_posterior\n</code></pre> <p>Note that this also works on the namespaces:</p> <pre><code>normal_model.optimize.methods\n\n['jaxopt_bfgs',\n 'jaxopt_gradient_descent',\n 'jaxopt_lbfgs',\n 'jaxopt_nonlinear_cg',\n 'optimistix_bfgs',\n 'optimistix_chord',\n 'optimistix_dogleg',\n 'optimistix_gauss_newton',\n 'optimistix_indirect_levenberg_marquardt',\n 'optimistix_levenberg_marquardt',\n 'optimistix_nelder_mead',\n 'optimistix_newton',\n 'optimistix_nonlinear_cg',\n 'optax_adabelief',\n 'optax_adafactor',\n 'optax_adagrad',\n 'optax_adam',\n 'optax_adamw',\n 'optax_adamax',\n 'optax_amsgrad',\n 'optax_fromage',\n 'optax_lamb',\n 'optax_lion',\n 'optax_noisy_sgd',\n 'optax_novograd',\n 'optax_radam',\n 'optax_rmsprop',\n 'optax_sgd',\n 'optax_sm3',\n 'optax_yogi']\n</code></pre> <p>and</p> <pre><code>print(normal_model.mcmc)\n\ntfp_hmc\ntfp_nuts\ntfp_snaper_hmc\nblackjax_hmc\nblackjax_chees_hmc\nblackjax_meads_hmc\nblackjax_nuts\nblackjax_hmc_pathfinder\nblackjax_nuts_pathfinder\nflowmc_rqspline_hmc\nflowmc_rqspline_mala\nflowmc_realnvp_hmc\nflowmc_realnvp_mala\nnumpyro_hmc\nnumpyro_nuts\n</code></pre>"},{"location":"examples/dynamax_and_bayeux/","title":"Using with Dynamax","text":"<pre><code>try:\n    import dynamax\nexcept ModuleNotFoundError:\n    print('installing dynamax')\n    %pip install -qq dynamax\n    import dynamax\n</code></pre> <pre><code>import bayeux as bx\nimport jax\nfrom itertools import count\n\nfrom dynamax.linear_gaussian_ssm import LinearGaussianSSM\nfrom dynamax.parameters import log_det_jac_constrain\nfrom dynamax.parameters import to_unconstrained, from_unconstrained\nfrom dynamax.utils.utils import ensure_array_has_batch_dim\nfrom functools import partial\n</code></pre> <pre><code>state_dim = 2\nemission_dim = 10\nnum_timesteps = 100\n\nseed = jax.random.key(0)\ninit_key, sample_key, test_init_key, inference_key = jax.random.split(seed, 4)\n\n# simulate synthetic data from true model\ntrue_model = LinearGaussianSSM(state_dim, emission_dim)\ntrue_params, _ = true_model.initialize(init_key)\ntrue_states, emissions = true_model.sample(true_params, sample_key, num_timesteps)\n\ntest_model = LinearGaussianSSM(state_dim, emission_dim)\ninitial_params, param_props = test_model.initialize(test_init_key)\n</code></pre> <pre><code>def dynamax_logdensity(\n        model,\n        props,\n        emissions,\n        inputs=None,\n    ):\n    \"\"\"Convert dynamax model into log-desnity function.\"\"\"\n    # Make sure the emissions and inputs have batch dimensions\n    batch_emissions = ensure_array_has_batch_dim(emissions, model.emission_shape)\n    batch_inputs = ensure_array_has_batch_dim(inputs, model.inputs_shape)\n\n    # log likelihood that the HMC samples from\n    def _logprob(unc_params):\n        params = from_unconstrained(unc_params, props)\n        batch_lls = jax.vmap(partial(model.marginal_log_prob, params))(batch_emissions, batch_inputs)\n        lp = model.log_prior(params) + batch_lls.sum()\n        lp += log_det_jac_constrain(params, props)\n        return lp\n\n    return _logprob\n</code></pre> <pre><code>log_density = dynamax_logdensity(test_model, param_props, emissions)\ninitial_unc_params = to_unconstrained(initial_params, param_props)\n\nssm_density = bx.Model(\n  log_density=log_density,\n  test_point=initial_unc_params\n)\n</code></pre> <p>We can use <code>debug</code> mode to help check if the model is correctly implemented.</p> <pre><code>ssm_density.mcmc.blackjax_hmc.debug(seed=inference_key)\n</code></pre> <pre>\n<code>Checking test_point shape \u2713 \nComputing test point log density \u2713 \nLoading keyword arguments... \u2713 \nChecking it is possible to compute an initial state \u2713 \nChecking initial state is has no NaN \u2713 \nComputing initial state log density \u2713 \nTransforming model to R^n \u2713 \nComputing transformed state log density shape \u2713 \nComputing gradients of transformed log density \u2713 \n</code>\n</pre> <pre>\n<code>True</code>\n</pre> <pre><code>samples = ssm_density.mcmc.blackjax_hmc(\n    seed=seed,\n    chain_method=\"vectorized\",\n    num_chains=2,\n    num_draws=500,\n    num_integration_steps=30,\n    progress_bar=True,\n    return_pytree=True,\n)\n\nconstrained_samples = from_unconstrained(samples, param_props)\n</code></pre> <pre>\n<code>Running window adaptation\n</code>\n</pre>        100.00% [500/500 00:00&lt;?]      <pre>\n<code>\n</code>\n</pre> <p>We are not just limited to <code>blackjax</code>, we can use another sampling backend like <code>numpyro</code>, or we can use gradient descent and other options, all with a simple interface thanks to <code>bayeux</code>.</p> <pre><code>print(ssm_density)\n</code></pre> <pre>\n<code>mcmc\n    .tfp_hmc\n    .tfp_nuts\n    .tfp_snaper_hmc\n    .blackjax_hmc\n    .blackjax_chees_hmc\n    .blackjax_meads_hmc\n    .blackjax_nuts\n    .blackjax_hmc_pathfinder\n    .blackjax_nuts_pathfinder\n    .flowmc_rqspline_hmc\n    .flowmc_rqspline_mala\n    .flowmc_realnvp_hmc\n    .flowmc_realnvp_mala\n    .numpyro_hmc\n    .numpyro_nuts\noptimize\n    .jaxopt_bfgs\n    .jaxopt_gradient_descent\n    .jaxopt_lbfgs\n    .jaxopt_nonlinear_cg\n    .optimistix_bfgs\n    .optimistix_chord\n    .optimistix_dogleg\n    .optimistix_gauss_newton\n    .optimistix_indirect_levenberg_marquardt\n    .optimistix_levenberg_marquardt\n    .optimistix_nelder_mead\n    .optimistix_newton\n    .optimistix_nonlinear_cg\n    .optax_adabelief\n    .optax_adafactor\n    .optax_adagrad\n    .optax_adam\n    .optax_adamw\n    .optax_adamax\n    .optax_amsgrad\n    .optax_fromage\n    .optax_lamb\n    .optax_lion\n    .optax_noisy_sgd\n    .optax_novograd\n    .optax_radam\n    .optax_rmsprop\n    .optax_sgd\n    .optax_sm3\n    .optax_yogi\nvi\n    .tfp_factored_surrogate_posterior\n</code>\n</pre>"},{"location":"examples/dynamax_and_bayeux/#using-bayeux-with-dynamax","title":"Using bayeux with Dynamax","text":"<p><code>dynamax</code> is a library for probabilistic state space models written in JAX. <code>dynamax</code> builds a pure JAX likelihood function from a model, and hence is compatible with other libraries in the JAX ecosystem: we can estimate model parameters using other JAX libraries such as <code>optax</code> (via stochastic gradient descent) and <code>blackjax</code> (via sampling).</p> <p>Here, we will provide minimal steps to recreate the inference stage for an example from the <code>dynamax</code> documentation for Bayesian parameter estimation for a linear Gaussian state space model using HMC. Writing inference loops in <code>blackjax</code>, especially for multiple chains, can be quite cumbersome. We will use <code>bayeux</code> to reduce some of the boilerplate code.</p> <p>This example shows how we can take any model in a JAX library and use <code>bayeux</code> to perform inference.</p>"},{"location":"examples/numpyro_and_bayeux/","title":"Using with NumPyro","text":"<pre><code>!pip install bayeux-ml\n</code></pre> <pre><code>import arviz as az\nimport bayeux as bx\nimport jax\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport numpyro\n\n\ndist = numpyro.distributions\n</code></pre> <pre><code>num_schools = 8\ntreatment_effects = np.array([28, 8, -3, 7, -1, 1, 18, 12], dtype=np.float32)\ntreatment_stddevs = np.array([15, 10, 16, 11, 9, 11, 10, 18], dtype=np.float32)\n\ndef numpyro_model():\n  avg_effect = numpyro.sample('avg_effect', dist.Normal(0., 10.))\n  avg_stddev = numpyro.sample('avg_stddev', dist.HalfNormal(10.))\n  with numpyro.plate('J', 8):\n    school_effects = numpyro.sample('school_effects', dist.Normal(0., 1.))\n    numpyro.sample('observed',\n                   dist.Normal(avg_effect[..., None] + avg_stddev[..., None] * school_effects,\n                               treatment_stddevs),\n                   obs=treatment_effects)\n\nbx_model = bx.Model.from_numpyro(numpyro_model)\n</code></pre> <pre><code>idata = bx_model.mcmc.blackjax_nuts(seed=jax.random.key(0))\n\naz.summary(idata)\n</code></pre> mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat avg_effect 6.498 4.225 -1.617 14.281 0.072 0.053 3499.0 2559.0 1.0 avg_stddev 4.588 3.638 0.002 11.262 0.075 0.053 1877.0 1610.0 1.0 school_effects[0] 0.342 0.962 -1.430 2.219 0.016 0.014 3832.0 2866.0 1.0 school_effects[1] 0.048 0.917 -1.720 1.753 0.015 0.015 3568.0 2978.0 1.0 school_effects[2] -0.164 0.949 -1.869 1.702 0.014 0.016 4428.0 2716.0 1.0 school_effects[3] 0.016 0.909 -1.744 1.697 0.013 0.014 4546.0 3120.0 1.0 school_effects[4] -0.259 0.929 -1.957 1.497 0.016 0.014 3436.0 2951.0 1.0 school_effects[5] -0.143 0.928 -1.878 1.597 0.014 0.015 4132.0 2834.0 1.0 school_effects[6] 0.337 0.935 -1.513 1.993 0.015 0.014 3757.0 3180.0 1.0 school_effects[7] 0.070 0.982 -1.797 1.901 0.016 0.016 3929.0 2696.0 1.0 <pre><code>opt_results = bx_model.optimize.optax_adam(seed=jax.random.PRNGKey(0))\n\n\nfig, ax = plt.subplots(figsize=(12, 2))\nax.plot(opt_results.loss.T)\nopt_results.params\n</code></pre> <pre>\n<code>{'avg_effect': Array([6.170515 , 6.170513 , 6.170513 , 6.1705136, 6.1705155, 6.1705165,\n        6.170514 , 6.1705165], dtype=float32),\n 'avg_stddev': Array([10.625167 , 10.625165 , 10.625165 , 10.6250305, 10.625165 ,\n        10.625165 , 10.625165 , 10.625165 ], dtype=float32),\n 'school_effects': Array([[ 0.68643355,  0.09130633, -0.26413605,  0.0376812 , -0.3929353 ,\n         -0.23488224,  0.5903885 ,  0.14177176],\n        [ 0.6864336 ,  0.09130642, -0.264136  ,  0.03768136, -0.39293545,\n         -0.23488215,  0.59038854,  0.1417718 ],\n        [ 0.6864337 ,  0.09130641, -0.26413602,  0.03768132, -0.39293548,\n         -0.23488215,  0.5903885 ,  0.14177178],\n        [ 0.68643266,  0.0913064 , -0.2653734 ,  0.03768122, -0.39293522,\n         -0.23488234,  0.5903885 ,  0.14177188],\n        [ 0.68643355,  0.0913063 , -0.26413605,  0.03768115, -0.39293563,\n         -0.23488225,  0.5903884 ,  0.14177172],\n        [ 0.68643355,  0.09130625, -0.26413608,  0.03768115, -0.39293572,\n         -0.2348823 ,  0.59038836,  0.1417717 ],\n        [ 0.6864336 ,  0.09130637, -0.26413602,  0.03768124, -0.3929355 ,\n         -0.23488219,  0.5903885 ,  0.14177176],\n        [ 0.68643355,  0.09130625, -0.26413608,  0.03768115, -0.3929357 ,\n         -0.2348823 ,  0.59038836,  0.1417717 ]], dtype=float32)}</code>\n</pre> <pre><code>surrogate_posterior, losses = bx_model.vi.tfp_factored_surrogate_posterior(\n    seed=jax.random.PRNGKey(0))\n\n\nfig, ax = plt.subplots(figsize=(12, 2))\nax.plot(losses.T)\n\ndraws = surrogate_posterior.sample(100, seed=jax.random.PRNGKey(1))\njax.tree.map(lambda x: np.mean(x, axis=(0, 1)), draws)\n</code></pre> <pre>\n<code>{'avg_effect': Array(6.4629807, dtype=float32),\n 'avg_stddev': Array(3.9471092, dtype=float32),\n 'school_effects': Array([ 0.36255646,  0.05804498, -0.17273399, -0.01884749, -0.21912189,\n        -0.16507226,  0.3044645 ,  0.06365629], dtype=float32)}</code>\n</pre>"},{"location":"examples/numpyro_and_bayeux/#using-bayeux-with-numpyro","title":"Using bayeux with numpyro","text":"<p><code>bayeux</code> has a built-in function <code>bx.Model.from_numpyro</code> that makes it easy to work with <code>numpyro</code> models.  More on NumPyro here</p> <p>We implement a common hierarchical model of the eight schools dataset (Rubin 1981\u00b9), whose details can be seen on the Stan documentation, PyMC documentation, TFP documentation, numpyro documentation, among others.</p> <p>\u00b9 Rubin, Donald B. 1981. \u201cEstimation in Parallel Randomized Experiments.\u201d Journal of Educational and Behavioral Statistics 6 (4): 377\u2013401.</p>"},{"location":"examples/oryx_and_bayeux/","title":"Using with Oryx","text":"<pre><code>!pip install -Uq bayeux-ml\n</code></pre> <pre><code>from sklearn import datasets\niris = datasets.load_iris()\nfeatures, labels = iris['data'], iris['target']\n\nnum_features = features.shape[-1]\nnum_classes = len(iris.target_names)\n\nimport functools\n\nimport bayeux as bx\nimport jax\nimport jax.numpy as jnp\nimport matplotlib.pyplot as plt\nimport oryx.core.ppl as oryx_ppl\nimport tensorflow_probability.substrates.jax as tfp\n\ntfd = tfp.distributions\n</code></pre> <pre><code>def dense(dim_out, activation=jax.nn.relu):\n  def forward(key, x):\n    dim_in = x.shape[-1]\n    w_key, b_key = jax.random.split(key)\n    w = oryx_ppl.random_variable(\n          tfd.Sample(tfd.Normal(0., 1.), sample_shape=(dim_out, dim_in)),\n          name='w')(w_key)\n    b = oryx_ppl.random_variable(\n          tfd.Sample(tfd.Normal(0., 1.), sample_shape=(dim_out,)),\n          name='b')(b_key)\n    return activation(jnp.dot(w, x) + b)\n  return forward\n\n\ndef mlp(hidden_sizes, num_classes):\n  num_hidden = len(hidden_sizes)\n  def forward(key, x):\n    keys = jax.random.split(key, num_hidden + 1)\n    for i, (subkey, hidden_size) in enumerate(zip(keys[:-1], hidden_sizes)):\n      x = oryx_ppl.nest(dense(hidden_size), scope=f'layer_{i + 1}')(subkey, x)\n    logits = oryx_ppl.nest(dense(num_classes, activation=lambda x: x),\n                  scope=f'layer_{num_hidden + 1}')(keys[-1], x)\n    return logits\n  return forward\n\ndef predict(mlp):\n  def forward(key, xs):\n    mlp_key, label_key = jax.random.split(key)\n    logits = jax.vmap(functools.partial(mlp, mlp_key))(xs)\n    return oryx_ppl.random_variable(\n        tfd.Independent(tfd.Categorical(logits=logits), 1), name='y')(label_key)\n  return forward\n</code></pre> <pre><code>(init_seed,\n bx_opt_seed,\n output_opt_seed,\n bx_mcmc_seed,\n output_mcmc_seed) = jax.random.split(jax.random.key(0), 5)\n\nbnn = mlp([200, 200], num_classes)\n\nweights = oryx_ppl.joint_sample(bnn)(init_seed, jnp.ones(num_features))\n\ndef target_log_prob(weights):\n  return oryx_ppl.joint_log_prob(predict(bnn))(dict(weights, y=labels), features)\n</code></pre> <pre><code>model = bx.Model(log_density=jax.jit(target_log_prob), test_point=weights)\nmodel.optimize.optax_adam.debug(seed=bx_opt_seed, verbosity=3,\n                                kwargs={'num_particles': 128})\n</code></pre> <pre>\n<code>Checking test_point shape \u2713 \nTest point has shape\n{'layer_1': {'b': (200,), 'w': (200, 4)}, 'layer_2': {'b': (200,), 'w': (200, 200)}, 'layer_3': {'b': (3,), 'w': (3, 200)}}\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nComputing test point log density \u2713 \nTest point has log density\nArray(-168474.19, dtype=float32)\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nLoading keyword arguments... \u2713 \nKeyword arguments are\n{&lt;function adam at 0x7ab5e9d4cd30&gt;: {'b1': 0.9,\n                                     'b2': 0.999,\n                                     'eps': 1e-08,\n                                     'eps_root': 0.0,\n                                     'learning_rate': 0.1,\n                                     'mu_dtype': None,\n                                     'nesterov': False},\n 'extra_parameters': {'apply_transform': True,\n                      'chain_method': 'vectorized',\n                      'num_iters': 1000,\n                      'num_particles': 128}}\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nChecking it is possible to compute an initial state \u2713 \nInitial state has shape\n{'layer_1': {'b': (128, 200), 'w': (128, 200, 4)}, 'layer_2': {'b': (128, 200), 'w': (128, 200, 200)}, 'layer_3': {'b': (128, 3), 'w': (128, 3, 200)}}\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nChecking initial state is has no NaN \u2713 \nNo nans detected!\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nComputing initial state log density \u2713 \nInitial state log density has shape\n(128,)\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nTransforming model to R^n \u2713 \nTransformed state has shape\n{'layer_1': {'b': (128, 200), 'w': (128, 200, 4)}, 'layer_2': {'b': (128, 200), 'w': (128, 200, 200)}, 'layer_3': {'b': (128, 3), 'w': (128, 3, 200)}}\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nComputing transformed state log density shape \u2713 \nTransformed state log density has shape\n(128,)\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\nComputing gradients of transformed log density \u2713 \nInitial gradient has shape\n{'layer_1': {'b': (128, 200), 'w': (128, 200, 4)}, 'layer_2': {'b': (128, 200), 'w': (128, 200, 200)}, 'layer_3': {'b': (128, 3), 'w': (128, 3, 200)}}\n\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\u2713\n\n</code>\n</pre> <pre>\n<code>True</code>\n</pre> <pre><code>%%time\nparams, state, loss = model.optimize.optax_adam(\n    seed=bx_opt_seed, num_particles=128)\n</code></pre> <pre>\n<code>CPU times: user 4.38 s, sys: 1.52 s, total: 5.89 s\nWall time: 8.81 s\n</code>\n</pre> <pre><code>output_logits = jax.vmap(lambda weights: jax.vmap(lambda x: oryx_ppl.intervene(bnn, **weights)(\n    output_opt_seed, x))(features))(params)\noutput_probs = jax.nn.softmax(output_logits)\n\nprint('Average sample accuracy:', (\n    output_probs.argmax(axis=-1) == labels[None]).mean())\nprint('BMA accuracy:', (\n    output_probs.mean(axis=0).argmax(axis=-1) == labels[None]).mean())\n</code></pre> <pre>\n<code>Average sample accuracy: 0.9825\nBMA accuracy: 0.9866667\n</code>\n</pre> <pre><code>%%time\nparams_mcmc = model.mcmc.numpyro_nuts(seed=bx_mcmc_seed,\n                                      step_size=1e-3,\n                                      max_tree_depth=8,\n                                      num_warmup=2_000,\n                                      return_pytree=True)\n</code></pre> <pre>\n<code>sample: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3000/3000 [20:23&lt;00:00,  2.45it/s]\n</code>\n</pre> <pre>\n<code>CPU times: user 20min 10s, sys: 3.89 s, total: 20min 14s\nWall time: 20min 34s\n</code>\n</pre> <pre><code>output_logits = jax.vmap(jax.vmap(lambda weights: jax.vmap(lambda x: oryx_ppl.intervene(bnn, **weights)(\n    output_mcmc_seed, x))(features)))(params_mcmc)\noutput_probs = jax.nn.softmax(output_logits)\n\nprint('Average sample accuracy:', (\n    output_probs.argmax(axis=-1) == labels[None]).mean())\nprint('BMA accuracy:', (\n    output_probs.mean(axis=0).argmax(axis=-1) == labels[None]).mean())\n</code></pre> <pre>\n<code>Average sample accuracy: 0.9882125\nBMA accuracy: 0.9919867\n</code>\n</pre> <pre><code>plt.plot(jax.vmap(jax.vmap(model.log_density))(params_mcmc).T);\n</code></pre>"},{"location":"examples/oryx_and_bayeux/#bayeux-and-oryx","title":"Bayeux and Oryx","text":"<p>Oryx is a library for probabilistic programming and deep learning built on top of Jax, and is used by <code>bayeux</code> for automatically computing inverse transforms and log determinant Jacobians. But <code>oryx</code> also includes tools for defining joint log densities.</p> <p>This notebook uses the example from the oryx documentation. The differences is that we additionally run an Adam optimizer, and use NumPyro's NUTS sampler (instead of TFP's HMC sampler).</p> <p>We load the iris dataset which classifies iris species by four measurements on the flowers, and will train a BNN on this data.</p>"},{"location":"examples/oryx_and_bayeux/#converting-to-a-bayeux-model","title":"Converting to a bayeux model","text":"<p>We convert the log density to a bayeux model, and run <code>model.debug</code> with a higher verbosity than normal to get a feel for what is going on. This lets us see that the biggest part of the model is <code>layer_2.w</code>, which is 200 x 200, meaning we'll be running inference on around 40,000 parameters, and that every posterior sample will cost us about 40kB of memory. Optimization with 128 particles will only be a few MB.</p>"},{"location":"examples/oryx_and_bayeux/#interpreting-the-optimization-results","title":"Interpreting the optimization results","text":"<p>An ensemble of optimization results is not a posterior, but with a BNN they'll mostly converge to different points, and it will be tempting to treat it as such. It is beyond the scope of this example to talk about how to capture uncertainty with an ensemble of point estimates, but we can still compute predictions in two different ways: taking an argmax, or computing a mean across particles.</p>"},{"location":"examples/oryx_and_bayeux/#running-mcmc","title":"Running MCMC","text":"<p>In order to make MCMC run in a reasonable amount of time, we: 1. Run on a GPU. 2. Limit the <code>max_tree_depth</code> to 8, since the No-U-Turn criterion is often not hit, especially during tuning, and this allows more frequent updating of tuning parameters. 3. Increase the <code>num_warmup</code> to 2,000, mostly to simulate burn-in. 4. Set <code>return_pytree</code> to <code>True</code>, since <code>arviz</code> is still not able to process nested dictionaries as of this being published.</p> <p>Note that by default we will get 8 chains of 500 draws each (~200MB), plus the sampler uses a few times this much internally, so we'll expect memory usage to be a few GB.</p>"},{"location":"examples/oryx_and_bayeux/#note","title":"Note:","text":"<p>I ran this with <code>max_tree_depth=7</code>, which makes the MCMC finish roughly twice as fast, and got the below plot of the 8 chains' log probabilities, which makes it more convincing that the chains were not quite mixing. I changed the tree depth to 8, and it did better, but I could also have increased the number of adaptation steps, probably. I have also fit this on TPUs, though you run into a matrix multiplication precision issue, and need to use <code>config.update('jax_default_matmul_precision', 'float32')</code> at the start of the colab.</p> <p></p>"},{"location":"examples/pymc_and_bayeux/","title":"Using with PyMC","text":"<pre><code>!pip install bayeux-ml\n</code></pre> <pre><code>import arviz as az\nimport bayeux as bx\nimport jax\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pymc as pm\n</code></pre> <pre><code>treatment_effects = np.array([28, 8, -3, 7, -1, 1, 18, 12], dtype=np.float32)\ntreatment_stddevs = np.array(\n    [15, 10, 16, 11, 9, 11, 10, 18], dtype=np.float32)\n\nwith pm.Model() as model:\n  avg_effect = pm.Normal('avg_effect', 0., 10.)\n  avg_stddev = pm.HalfNormal('avg_stddev', 10.)\n  school_effects = pm.Normal('school_effects', shape=8)\n  pm.Normal('observed',\n            avg_effect + avg_stddev * school_effects,\n            treatment_stddevs,\n            observed=treatment_effects)\n\nbx_model = bx.Model.from_pymc(model)\n</code></pre> <pre><code>idata = bx_model.mcmc.blackjax_nuts(seed=jax.random.key(0))\n\naz.summary(idata)\n</code></pre> mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat avg_effect 6.392 4.207 -1.522 13.967 0.069 0.049 3717.0 2153.0 1.00 avg_stddev 4.681 3.728 0.001 11.391 0.075 0.053 1835.0 1255.0 1.00 school_effects[0] 0.344 0.973 -1.571 2.073 0.016 0.015 3784.0 2729.0 1.01 school_effects[1] 0.051 0.901 -1.784 1.671 0.014 0.014 4265.0 2982.0 1.00 school_effects[2] -0.126 0.943 -1.917 1.594 0.015 0.015 4037.0 2689.0 1.00 school_effects[3] 0.020 0.911 -1.710 1.733 0.014 0.015 4367.0 3061.0 1.00 school_effects[4] -0.245 0.899 -1.942 1.423 0.014 0.013 4306.0 2970.0 1.00 school_effects[5] -0.131 0.921 -1.916 1.554 0.015 0.014 4039.0 2874.0 1.00 school_effects[6] 0.362 0.907 -1.350 2.034 0.013 0.013 4549.0 3204.0 1.00 school_effects[7] 0.066 0.972 -1.678 1.942 0.015 0.016 4481.0 3004.0 1.00 <pre><code>opt_results = bx_model.optimize.optax_adam(seed=jax.random.PRNGKey(0))\n\n\nfig, ax = plt.subplots(figsize=(12, 2))\nax.plot(opt_results.loss.T)\nopt_results.params\n</code></pre> <pre>\n<code>{'avg_effect': Array([6.17051501, 6.17051501, 6.17051501, 6.17051501, 6.17051501,\n        6.17051501, 6.1705141 , 6.17051501], dtype=float64),\n 'avg_stddev': Array([10.6251654 , 10.6251654 , 10.6251654 , 10.6251654 , 10.6251654 ,\n        10.6251654 , 10.62516158, 10.6251654 ], dtype=float64),\n 'school_effects': Array([[ 0.68643359,  0.09130632, -0.26413604,  0.03768121, -0.39293559,\n         -0.23488223,  0.59038842,  0.14177174],\n        [ 0.68643359,  0.09130632, -0.26413604,  0.03768121, -0.39293559,\n         -0.23488223,  0.59038842,  0.14177174],\n        [ 0.68643359,  0.09130632, -0.26413604,  0.03768121, -0.39293559,\n         -0.23488223,  0.59038842,  0.14177174],\n        [ 0.68643359,  0.09130632, -0.26413604,  0.03768121, -0.39293559,\n         -0.23488223,  0.59038842,  0.14177174],\n        [ 0.68643359,  0.09130632, -0.26413604,  0.03768121, -0.39293559,\n         -0.23488223,  0.59038842,  0.14177174],\n        [ 0.68643359,  0.09130632, -0.26413604,  0.03768121, -0.39293559,\n         -0.23488223,  0.59038842,  0.14177174],\n        [ 0.68643307,  0.09130638, -0.26413599,  0.03768124, -0.39293562,\n         -0.23494317,  0.59038848,  0.14177166],\n        [ 0.68643359,  0.09130632, -0.26413604,  0.03768121, -0.39293559,\n         -0.23488223,  0.59038842,  0.14177174]], dtype=float64)}</code>\n</pre> <pre><code>surrogate_posterior, losses = bx_model.vi.tfp_factored_surrogate_posterior(\n    seed=jax.random.PRNGKey(0))\n\n\nfig, ax = plt.subplots(figsize=(12, 2))\nax.plot(losses.T)\n\ndraws = surrogate_posterior.sample(100, seed=jax.random.PRNGKey(1))\njax.tree.map(lambda x: np.mean(x, axis=(0, 1)), draws)\n</code></pre> <pre>\n<code>{'avg_effect': Array(6.462984, dtype=float32),\n 'avg_stddev': Array(3.947107, dtype=float32),\n 'school_effects': Array([ 0.36255622,  0.05804485, -0.17273399, -0.01884761, -0.2191218 ,\n        -0.16507229,  0.3044642 ,  0.06365623], dtype=float32)}</code>\n</pre>"},{"location":"examples/pymc_and_bayeux/#using-bayeux-with-pymc","title":"Using bayeux with pymc","text":"<p><code>bayeux</code> has a built-in function <code>bx.Model.from_pymc</code> that makes it easy to work with <code>pymc</code> models.  More on PyMC here.</p> <p>We implement a common hierarchical model of the eight schools dataset (Rubin 1981\u00b9), whose details can be seen on the Stan documentation, PyMC documentation, TFP documentation, numpyro documentation, among others.</p> <p>\u00b9 Rubin, Donald B. 1981. \u201cEstimation in Parallel Randomized Experiments.\u201d Journal of Educational and Behavioral Statistics 6 (4): 377\u2013401.</p>"},{"location":"examples/tfp_and_bayeux/","title":"Using with TFP","text":"<pre><code>import arviz as az\nimport bayeux as bx\nimport jax\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport tensorflow_probability.substrates.jax as tfp\n\ntfd = tfp.distributions\n</code></pre> <pre><code>num_schools = 8\ntreatment_effects = np.array([28, 8, -3, 7, -1, 1, 18, 12], dtype=np.float32)\ntreatment_stddevs = np.array([15, 10, 16, 11, 9, 11, 10, 18], dtype=np.float32)\n\n@tfd.JointDistributionCoroutineAutoBatched\ndef tfp_model():\n  avg_effect = yield tfd.Normal(0., 10., name='avg_effect')\n  avg_stddev = yield tfd.HalfNormal(10., name='avg_stddev')\n  school_effects = yield tfd.Sample(\n      tfd.Normal(0., 1.), sample_shape=8, name='school_effects')\n\n  yield tfd.Normal(loc=avg_effect + avg_stddev * school_effects,\n                   scale=treatment_stddevs, name='observed')\n\nbx_model = bx.Model.from_tfp(\n    tfp_model.experimental_pin(observed=treatment_effects))\n</code></pre> <pre><code>idata = bx_model.mcmc.blackjax_nuts(seed=jax.random.key(0))\n\naz.summary(idata)\n</code></pre> mean sd hdi_3% hdi_97% mcse_mean mcse_sd ess_bulk ess_tail r_hat avg_effect 6.593 4.070 -1.180 14.317 0.054 0.043 5630.0 3278.0 1.0 avg_stddev 4.617 3.548 0.003 10.873 0.069 0.054 3387.0 2334.0 1.0 school_effects[0] 0.341 0.973 -1.408 2.201 0.012 0.017 6129.0 2774.0 1.0 school_effects[1] 0.055 0.912 -1.654 1.741 0.011 0.016 7001.0 2885.0 1.0 school_effects[2] -0.140 0.952 -1.973 1.590 0.012 0.015 6433.0 3189.0 1.0 school_effects[3] -0.000 0.933 -1.766 1.723 0.011 0.016 6824.0 3069.0 1.0 school_effects[4] -0.256 0.917 -1.879 1.543 0.013 0.014 4907.0 3054.0 1.0 school_effects[5] -0.139 0.942 -1.935 1.646 0.012 0.016 6657.0 3072.0 1.0 school_effects[6] 0.338 0.944 -1.617 1.973 0.013 0.015 5215.0 2890.0 1.0 school_effects[7] 0.056 0.954 -1.692 1.870 0.011 0.016 7033.0 3048.0 1.0 <pre><code>opt_results = bx_model.optimize.optax_adam(seed=jax.random.key(0))\n\nfig, ax = plt.subplots(figsize=(12, 2))\nax.plot(opt_results.loss.T)\nopt_results.params\n</code></pre> <pre>\n<code>StructTuple(\n  avg_effect=Array([6.170572 , 6.170516 , 6.1705165, 6.1705136, 6.170514 , 6.170519 ,\n           6.1705165, 6.170516 ], dtype=float32),\n  avg_stddev=Array([10.624942, 10.625168, 10.625167, 10.625159, 10.625164, 10.625158,\n           10.625162, 10.625166], dtype=float32),\n  school_effects=Array([[ 0.6820582 ,  0.09130666, -0.2641336 ,  0.03768046, -0.3825404 ,\n            -0.2348828 ,  0.5903886 ,  0.14177026],\n           [ 0.68643355,  0.09130628, -0.2641361 ,  0.03768098, -0.39293563,\n            -0.23488228,  0.59038836,  0.14177173],\n           [ 0.68643355,  0.09130625, -0.2641361 ,  0.0376812 , -0.39293563,\n            -0.23488231,  0.5903883 ,  0.1417717 ],\n           [ 0.6864335 ,  0.09130641, -0.26382452,  0.03768129, -0.39293548,\n            -0.23488213,  0.59038854,  0.14177175],\n           [ 0.68643355,  0.0913064 , -0.26413602,  0.03768126, -0.39293554,\n            -0.23488219,  0.59038854,  0.14177175],\n           [ 0.6864333 ,  0.09130613, -0.26413608,  0.03768104, -0.39293587,\n            -0.2348824 ,  0.5903882 ,  0.1417716 ],\n           [ 0.6864335 ,  0.09130625, -0.26413605,  0.03768114, -0.3929357 ,\n            -0.23488227,  0.59038836,  0.14177169],\n           [ 0.68643355,  0.09130628, -0.26413608,  0.03768117, -0.39293563,\n            -0.23488228,  0.59038836,  0.14177173]], dtype=float32)\n)</code>\n</pre> <pre><code>fit_key, draw_key = jax.random.split(jax.random.key(0))\nsurrogate_posterior, losses = bx_model.vi.tfp_factored_surrogate_posterior(\n    seed=fit_key)\n\n\nfig, ax = plt.subplots(figsize=(12, 2))\nax.plot(losses.T)\n\ndraws = surrogate_posterior.sample(100, seed=draw_key)\njax.tree.map(lambda x: np.mean(x, axis=(0, 1)), draws)\n</code></pre> <pre>\n<code>StructTuple(\n  avg_effect=Array(6.5087867, dtype=float32),\n  avg_stddev=Array(3.7763548, dtype=float32),\n  school_effects=Array([ 0.3577297 ,  0.06299979, -0.08964223, -0.02790713, -0.26249716,\n           -0.1486009 ,  0.36491287,  0.05022464], dtype=float32)\n)</code>\n</pre>"},{"location":"examples/tfp_and_bayeux/#using-bayeux-with-tfp-on-jax","title":"Using bayeux with TFP-on-JAX","text":"<p><code>bayeux</code> has a built-in function for working with TFP models, <code>bx.Model.from_tfp</code>. More on TensorFlow Probability here.</p> <p>We implement a common hierarchical model of the eight schools dataset (Rubin 1981\u00b9), whose details can be seen on the Stan documentation, PyMC documentation, TFP documentation, numpyro documentation, among others.</p> <p>\u00b9 Rubin, Donald B. 1981. \u201cEstimation in Parallel Randomized Experiments.\u201d Journal of Educational and Behavioral Statistics 6 (4): 377\u2013401.</p>"}]}